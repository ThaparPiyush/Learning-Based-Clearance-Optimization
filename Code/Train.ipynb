{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cuda\n"
     ]
    }
   ],
   "source": [
    "# check for cuda\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Device: \", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load and preprocess the data\n",
    "def load_data(file_path):\n",
    "    # Load data from CSV\n",
    "    data = pd.read_csv(file_path)\n",
    "    data = data[data.iloc[:, -1] >= 0]\n",
    "    for col in [4, 7, 8]:  # Columns with potential mixed types\n",
    "        data.iloc[:, col] = pd.to_numeric(data.iloc[:, col], errors='coerce')\n",
    "\n",
    "    # Drop rows with NaN values in the critical columns\n",
    "    data = data.dropna(subset=data.columns[[4, 7, 8]])\n",
    "\n",
    "    # Convert all remaining columns to numeric\n",
    "    data = data.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    # Drop any rows with NaN after conversion\n",
    "    data = data.dropna()\n",
    "\n",
    "    # Split into input features and output (last column is the output)\n",
    "    X = data.iloc[:, :-1].values  # All columns except the last one\n",
    "    y = data.iloc[:, -1].values   # Last column is the target\n",
    "\n",
    "    # Normalize X[:, 0] and X[:, 2] by their respective max values\n",
    "    X[:, 0] = X[:, 0] / 0.38615\n",
    "    X[:, 1] = X[:, 1] / 1.6056\n",
    "    X[:, 2] = X[:, 2] / 1.518\n",
    "    X[:, 3] = X[:, 3] / 3.14\n",
    "    X[:, 4] = X[:, 4] / 2.251\n",
    "    X[:, 5] = X[:, 5] / 3.14\n",
    "    X[:, 6] = X[:, 6] / 2.16\n",
    "    X[:, 7] = X[:, 7] / 3.14\n",
    "\n",
    "    print(\"y: \", np.max(y))\n",
    "    y = y / np.max(y) \n",
    "\n",
    "    print(X.shape)\n",
    "\n",
    "    # Split into train, validation, and test sets (85%, 5%, 10%)\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.4, random_state=42)\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
    "    X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "    y_val_tensor = torch.tensor(y_val, dtype=torch.float32).view(-1, 1)\n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "    y_test_tensor = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "    # Create DataLoader for batching\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=191, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=191, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=191, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10, min_delta=1e-4):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "        \n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss > self.best_loss - self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "def train(model, train_loader, val_loader, criterion, optimizer, epochs=100):\n",
    "    model.train()  # Set the model to training mode\n",
    "    early_stopping = EarlyStopping(patience=8, min_delta=1e-4)\n",
    "    # scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=8, verbose=True, min_lr=1e-6)\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0.0\n",
    "        for inputs, target in train_loader:\n",
    "            optimizer.zero_grad()  # Zero the gradients\n",
    "            inputs, target = inputs.to(device), target.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, target)\n",
    "\n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # scheduler.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        # Validation step\n",
    "        val_loss = 0.0\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        with torch.no_grad():\n",
    "            for inputs, target in val_loader:\n",
    "                inputs, target = inputs.to(device), target.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, target)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        # scheduler.step(avg_val_loss)\n",
    "        # Print training and validation loss\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {epoch_loss / len(train_loader)}, Validation Loss: {val_loss / len(val_loader)}\")\n",
    "        if early_stopping(val_loss/len(val_loader)):\n",
    "            print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming the CSV file is located at 'data.csv'\n",
    "train_loader, val_loader, test_loader = load_data('dec11_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimensions of datasets\n",
    "print(len(train_loader.dataset), len(val_loader.dataset), len(test_loader.dataset))\n",
    "\n",
    "# initial values of dataset\n",
    "print(train_loader.dataset[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model and calculate error percentages\n",
    "def test(model, test_loader, criterion):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    all_errors = []\n",
    "    actuals = []\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, target in test_loader:\n",
    "            inputs, target = inputs.to(device), target.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, target)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            # Calculate the error percentage\n",
    "            error_percent = (torch.abs(outputs - target) / torch.abs(target)) * 100\n",
    "            all_errors.extend(error_percent.squeeze().tolist())\n",
    "            actuals.extend(target.squeeze().tolist())\n",
    "            predictions.extend(outputs.squeeze().tolist())\n",
    "\n",
    "    # Average test loss\n",
    "    avg_test_loss = test_loss / len(test_loader)\n",
    "    print(f\"Test Loss: {avg_test_loss}\")\n",
    "\n",
    "    errors_array = np.array(all_errors)\n",
    "\n",
    "    mid_error_count = np.sum(errors_array > 10)\n",
    "    mid_error_percentage = (mid_error_count / len(errors_array)) * 100\n",
    "    print(f\"Percentage of test samples with error > 10%: {mid_error_percentage:.2f}%\")\n",
    "    high_error_count = np.sum(errors_array > 20)\n",
    "    high_error_percentage = (high_error_count / len(errors_array)) * 100\n",
    "    print(f\"Percentage of test samples with error > 20%: {high_error_percentage:.2f}%\")\n",
    "\n",
    "    # Remove outliers from error percentages using IQR\n",
    "    Q1 = np.percentile(errors_array, 25)\n",
    "    Q3 = np.percentile(errors_array, 75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    filtered_errors = errors_array[(errors_array >= lower_bound) & (errors_array <= upper_bound)]\n",
    "\n",
    "    # Plotting the filtered error percentages\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(filtered_errors, label=\"Error Percentage (Filtered)\", marker='o', linestyle='', color='b')\n",
    "    plt.xlabel(\"Filtered Test Sample Index\")\n",
    "    plt.ylabel(\"Error Percentage (%)\")\n",
    "    plt.title(\"Filtered Error Percentage in Predicted vs Actual Values\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # Optional: Scatter plot of predictions vs actuals\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.scatter(actuals, predictions, label=\"Predicted vs Actual\", color='r', alpha=0.5)\n",
    "    plt.plot([min(actuals), max(actuals)], [min(actuals), max(actuals)], 'g--', label=\"Ideal Fit (y=x)\")\n",
    "    plt.xlabel(\"Actual Values\")\n",
    "    plt.ylabel(\"Predicted Values\")\n",
    "    plt.title(\"Predicted vs Actual Values\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    random_indices = np.random.choice(len(errors_array), 5)\n",
    "\n",
    "    for i in random_indices:\n",
    "        print(f\"Actual: {actuals[i]}, Predicted: {predictions[i]}, Error: {all_errors[i]:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy_test(model, test_loader, criterion):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    all_errors = []\n",
    "    abs_errors = []\n",
    "    actuals = []\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, target in test_loader:\n",
    "            inputs, target = inputs.to(device), target.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, target)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            # Calculate the error percentage\n",
    "            error_percent = (torch.abs(outputs - target) / torch.abs(target)) * 100\n",
    "            all_errors.extend(error_percent.squeeze().tolist())\n",
    "            abs_errors.extend(torch.abs(outputs - target).squeeze().tolist())\n",
    "            actuals.extend(target.squeeze().tolist())\n",
    "            predictions.extend(outputs.squeeze().tolist())\n",
    "\n",
    "    # Average test loss\n",
    "    avg_test_loss = test_loss / len(test_loader)\n",
    "    print(f\"Test Loss: {avg_test_loss}\")\n",
    "\n",
    "    errors_array = np.array(all_errors)\n",
    "    abs_errors_array = np.array(abs_errors)\n",
    "\n",
    "    mid_error_count = np.sum(errors_array > 10)\n",
    "    mid_error_percentage = (mid_error_count / len(errors_array)) * 100\n",
    "    print(f\"Percentage of test samples with error > 10%: {mid_error_percentage:.2f}%\")\n",
    "    high_error_count = np.sum(errors_array > 20)\n",
    "    high_error_percentage = (high_error_count / len(errors_array)) * 100\n",
    "    print(f\"Percentage of test samples with error > 20%: {high_error_percentage:.2f}%\")\n",
    "\n",
    "    # Filter errors less than 20%\n",
    "    low_error_mask = abs_errors_array <= 0.1\n",
    "    low_error_actuals = np.array(actuals)[low_error_mask]\n",
    "    low_error_predictions = np.array(predictions)[low_error_mask]\n",
    "\n",
    "    # Optional: Scatter plot of predictions vs actuals with errors < 20%\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.scatter(low_error_actuals, low_error_predictions, label=\"Predicted vs Actual (Error < 20%)\", color='r', alpha=0.5)\n",
    "    plt.plot([min(low_error_actuals), max(low_error_actuals)], \n",
    "             [min(low_error_actuals), max(low_error_actuals)], \n",
    "             'g--', label=\"Ideal Fit (y=x)\")\n",
    "    plt.xlabel(\"Actual Values\")\n",
    "    plt.ylabel(\"Predicted Values\")\n",
    "    plt.title(\"Predicted vs Actual Values (Errors < 20%)\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # Print details of 5 random samples with low errors\n",
    "    low_error_indices = np.where(low_error_mask)[0]\n",
    "    random_indices = np.random.choice(low_error_indices, min(5, len(low_error_indices)))\n",
    "\n",
    "    for i in random_indices:\n",
    "        print(f\"Actual: {actuals[i]}, Predicted: {predictions[i]}, Error: {all_errors[i]:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network model\n",
    "class DistancePredictor(nn.Module):\n",
    "    # def __init__(self, input_size=14, hidden_layers=[32, 64, 32, 16], output_size=1): # 19.42%, lr=0.002, epochs=150\n",
    "    # def __init__(self, input_size=14, hidden_layers=[32, 64, 128], output_size=1): # 18.31%, lr=0.001, epochs=150\n",
    "    # def __init__(self, input_size=14, hidden_layers=[32, 64, 128, 64, 32], output_size=1): # 16.6%, lr=0.005, epochs=300\n",
    "    # def __init__(self, input_size=14, hidden_layers=[32, 64], output_size=1): # 21.13%, lr=0.01, epochs=100\n",
    "    # def __init__(self, input_size=14, hidden_layers=[16, 16], output_size=1): # 24.72%, lr=0.01, epochs=100\n",
    "    # def __init__(self, input_size=14, hidden_layers=[16, 32, 16], output_size=1): # 21.51%, lr=0.005, epochs=300\n",
    "    # def __init__(self, input_size=14, hidden_layers=[32, 128, 512, 1024, 256, 16], output_size=1): # 15.8%, lr=0.003, epochs=200\n",
    "    # def __init__(self, input_size=14, hidden_layers=[32, 128, 512, 256, 16], output_size=1): # 14.78%%, lr=0.003, epochs=200\n",
    "    # def __init__(self, input_size=14, hidden_layers=[32, 128, 512, 256, 16, 4], output_size=1): # 16.54%, lr=0.002, epochs=150\n",
    "    # def __init__(self, input_size=14, hidden_layers=[256, 256, 64, 32], output_size=1): # 21.13%, lr=0.01, epochs=100\n",
    "    # def __init__(self, input_size=14, hidden_layers=[128,256,128], output_size=1):\n",
    "    def __init__(self, input_size=14, hidden_layers=[1400, 1400], output_size=1): \n",
    "        super(DistancePredictor, self).__init__()\n",
    "        \n",
    "        # Define the network layers\n",
    "        layers = []\n",
    "        in_features = input_size\n",
    "        for hidden_units in hidden_layers:\n",
    "            layers.append(nn.Linear(in_features, hidden_units))\n",
    "            layers.append(nn.ReLU())  # Activation function\n",
    "            # add a dropout layer\n",
    "            layers.append(nn.Dropout(0.01))\n",
    "            in_features = hidden_units\n",
    "        layers.append(nn.Linear(in_features, output_size))  # Output layer\n",
    "\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedMSELoss(nn.Module):\n",
    "    def __init__(self, weight_factor=10):\n",
    "        super(WeightedMSELoss, self).__init__()\n",
    "        self.weight_factor = weight_factor\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        weight = torch.where(targets < 0.5, self.weight_factor, 1.0)\n",
    "        weight = torch.where(targets < 0.25, self.weight_factor*3, self.weight_factor)\n",
    "        # weight = torch.where(targets > 0.5, 1.0, weight)\n",
    "        return torch.mean(weight * (inputs - targets) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class WeightedMSELoss(torch.nn.Module):\n",
    "#     def __init__(self, positive_weight=1.0, negative_weight=9.5):\n",
    "#         super(WeightedMSELoss, self).__init__()\n",
    "#         self.positive_weight = positive_weight\n",
    "#         self.negative_weight = negative_weight\n",
    "\n",
    "#     def forward(self, y_pred, y_true):\n",
    "#         weights = torch.where(y_true >= 0, self.positive_weight, self.negative_weight)\n",
    "#         loss = weights * (y_pred - y_true) ** 2\n",
    "#         return loss.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "model = DistancePredictor().to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "# criterion = nn.MSELoss().to(device)\n",
    "# criterion = nn.HuberLoss().to(device)\n",
    "# criterion = WeightedMSELoss(weight_factor=70).to(device)  # Adjust the weight factor as needed\n",
    "criterion = WeightedMSELoss().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1.7495e-04)  # Adjust the learning rate as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"model_cnrrt_dec11_new_weight.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300, Train Loss: 0.09832068136551532, Validation Loss: 0.07408497602016392\n",
      "Epoch 2/300, Train Loss: 0.06424453103938287, Validation Loss: 0.05746081264904235\n",
      "Epoch 3/300, Train Loss: 0.05185185428620931, Validation Loss: 0.046427000076558804\n",
      "Epoch 4/300, Train Loss: 0.044387859131901373, Validation Loss: 0.04237768365009509\n",
      "Epoch 5/300, Train Loss: 0.03956249532462921, Validation Loss: 0.039257396388217826\n",
      "Epoch 6/300, Train Loss: 0.03601262263272278, Validation Loss: 0.03692296997842415\n",
      "Epoch 7/300, Train Loss: 0.033228756908221906, Validation Loss: 0.03558115279833256\n",
      "Epoch 8/300, Train Loss: 0.03111396458017517, Validation Loss: 0.034922684098315254\n",
      "Epoch 9/300, Train Loss: 0.029330702322508534, Validation Loss: 0.03071325433053449\n",
      "Epoch 10/300, Train Loss: 0.027905381127562606, Validation Loss: 0.028836127880167074\n",
      "Epoch 11/300, Train Loss: 0.026621661994867887, Validation Loss: 0.02783529499840594\n",
      "Epoch 12/300, Train Loss: 0.025533222536908515, Validation Loss: 0.027436711806457\n",
      "Epoch 13/300, Train Loss: 0.02457286614056151, Validation Loss: 0.027417693207445918\n",
      "Epoch 14/300, Train Loss: 0.023736631686559727, Validation Loss: 0.026492091324570675\n",
      "Epoch 15/300, Train Loss: 0.022910600882126077, Validation Loss: 0.02484204926082433\n",
      "Epoch 16/300, Train Loss: 0.02227811906827616, Validation Loss: 0.02744191006309547\n",
      "Epoch 17/300, Train Loss: 0.02163140659752887, Validation Loss: 0.02524471512049159\n",
      "Epoch 18/300, Train Loss: 0.021004721064479783, Validation Loss: 0.02379304279949477\n",
      "Epoch 19/300, Train Loss: 0.02050015844844326, Validation Loss: 0.02386629345639239\n",
      "Epoch 20/300, Train Loss: 0.019982200220294274, Validation Loss: 0.02322053246642436\n",
      "Epoch 21/300, Train Loss: 0.019587250042875617, Validation Loss: 0.02369656677361105\n",
      "Epoch 22/300, Train Loss: 0.019095508851900703, Validation Loss: 0.022236870535637433\n",
      "Epoch 23/300, Train Loss: 0.018778737342365185, Validation Loss: 0.0217102984694506\n",
      "Epoch 24/300, Train Loss: 0.018369189479012775, Validation Loss: 0.02242686752012006\n",
      "Epoch 25/300, Train Loss: 0.01798487152591228, Validation Loss: 0.02258973517422635\n",
      "Epoch 26/300, Train Loss: 0.017711638319258892, Validation Loss: 0.02123868545084732\n",
      "Epoch 27/300, Train Loss: 0.017338805961197114, Validation Loss: 0.021032634770987737\n",
      "Epoch 28/300, Train Loss: 0.017124307412040112, Validation Loss: 0.020817421899828735\n",
      "Epoch 29/300, Train Loss: 0.016788522358311783, Validation Loss: 0.02096191714344301\n",
      "Epoch 30/300, Train Loss: 0.01646849980545611, Validation Loss: 0.02054815846186335\n",
      "Epoch 31/300, Train Loss: 0.016226436356427492, Validation Loss: 0.021122741370111688\n",
      "Epoch 32/300, Train Loss: 0.01597695001095122, Validation Loss: 0.020495386019552926\n",
      "Epoch 33/300, Train Loss: 0.015740402651788662, Validation Loss: 0.019622603302324956\n",
      "Epoch 34/300, Train Loss: 0.015522682977845837, Validation Loss: 0.021638688217735784\n",
      "Epoch 35/300, Train Loss: 0.015316690153018965, Validation Loss: 0.0201272148998374\n",
      "Epoch 36/300, Train Loss: 0.015132030454790642, Validation Loss: 0.020042857727387158\n",
      "Epoch 37/300, Train Loss: 0.014908494484340114, Validation Loss: 0.0198271694724319\n",
      "Epoch 38/300, Train Loss: 0.014713869841483302, Validation Loss: 0.020038868707460508\n",
      "Epoch 39/300, Train Loss: 0.014559085994894121, Validation Loss: 0.019132374380894084\n",
      "Epoch 40/300, Train Loss: 0.014300330902581903, Validation Loss: 0.019330239935525465\n",
      "Epoch 41/300, Train Loss: 0.014207283090158989, Validation Loss: 0.01917929543541122\n",
      "Epoch 42/300, Train Loss: 0.014003567823311698, Validation Loss: 0.018897379276627834\n",
      "Epoch 43/300, Train Loss: 0.013884364980961027, Validation Loss: 0.01935966650530375\n",
      "Epoch 44/300, Train Loss: 0.013696101722042294, Validation Loss: 0.01902481195396539\n",
      "Epoch 45/300, Train Loss: 0.013542522310731955, Validation Loss: 0.01878664455576057\n",
      "Epoch 46/300, Train Loss: 0.013420384780464435, Validation Loss: 0.018525412572088094\n",
      "Epoch 47/300, Train Loss: 0.013244227338797048, Validation Loss: 0.01845072174625855\n",
      "Epoch 48/300, Train Loss: 0.013074403152652873, Validation Loss: 0.018700024513939744\n",
      "Epoch 49/300, Train Loss: 0.012950370692017349, Validation Loss: 0.017916462274118652\n",
      "Epoch 50/300, Train Loss: 0.012858553860576695, Validation Loss: 0.01816585563189362\n",
      "Epoch 51/300, Train Loss: 0.012651352818478042, Validation Loss: 0.01818994157763287\n",
      "Epoch 52/300, Train Loss: 0.012581655593381662, Validation Loss: 0.018137720364717305\n",
      "Epoch 53/300, Train Loss: 0.012445636117412071, Validation Loss: 0.018168374706315563\n",
      "Epoch 54/300, Train Loss: 0.012336732207153644, Validation Loss: 0.017810052877933878\n",
      "Epoch 55/300, Train Loss: 0.012236303226220317, Validation Loss: 0.018186588217347116\n",
      "Epoch 56/300, Train Loss: 0.012065524263556391, Validation Loss: 0.01756215198138974\n",
      "Epoch 57/300, Train Loss: 0.01197140187568179, Validation Loss: 0.01786567826649094\n",
      "Epoch 58/300, Train Loss: 0.011862140065654565, Validation Loss: 0.018069706009364795\n",
      "Epoch 59/300, Train Loss: 0.011763200019501706, Validation Loss: 0.017569341243605745\n",
      "Epoch 60/300, Train Loss: 0.011721381554174237, Validation Loss: 0.017984177647548814\n",
      "Epoch 61/300, Train Loss: 0.011576940982586591, Validation Loss: 0.017682147985356788\n",
      "Epoch 62/300, Train Loss: 0.01145733684386959, Validation Loss: 0.01776492882685842\n",
      "Epoch 63/300, Train Loss: 0.011377837534664897, Validation Loss: 0.018104807410293775\n",
      "Epoch 64/300, Train Loss: 0.011260788623730221, Validation Loss: 0.018074152714164106\n",
      "Early stopping triggered at epoch 64\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "train(model, train_loader, val_loader, criterion, optimizer, epochs=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model on test set\n",
    "# test(model, test_loader, criterion)\n",
    "dummy_test(model, test_loader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving model's state dictionary\n",
    "torch.save(model.state_dict(), \"model_cnrrt_dec11_new_weight.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
